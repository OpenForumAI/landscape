{"classification":{"category":{"name":"Technical Prototypes","normalized_name":"technical-prototypes","subcategories":[{"name":"Data","normalized_name":"data"}]}},"foundation":"OFAI","items":[{"category":"Technical Prototypes","id":"technical-prototypes--data--dare-analysis-tools","name":"DARE Analysis Tools","logo":"logos/a057e0b05e78ad13e83a0615596f9df5a60e12969d2b76f3517058cbeefe6ebd.svg","subcategory":"Data","website":"https://www.cmu.edu/dietrich/ai/education/dare.html","description":"The D.A.R.E. system analyzes qualitative datasets to reduce administrative burden and increase efficiency, from surveys to transcripts. It quickly identifies themes, changes, and sentiment, while keeping human experts in the loop for validation. The system incorporates additional data for correlative and longitudinal analysis."},{"category":"Technical Prototypes","id":"technical-prototypes--data--creative-commons-open-infrastructure-of-sharing","name":"Creative Commons Open Infrastructure of Sharing","logo":"logos/df248137053c6bef73c0964cc95495c9aeba3cc021b5ae9a5ed5c9a0524a6bd0.svg","subcategory":"Data","website":"https://creativecommons.org/share-your-work/cclicenses/","description":"CC serves as the legal layer of the open infrastructure of sharing, which is an essential digital public good. Our goal is to enable a strong and resilient open infrastructure that empowers sharing and access in the public interest. The CC licenses ensure that humans and machines benefit from the commons in ways that serve the public good. The commons that are fostered by open practices and open licenses are also an essential part of ensuring open datasets for a public AI ecosystem."},{"category":"Technical Prototypes","id":"technical-prototypes--data--eva-db","name":"Eva DB","logo":"logos/38b2ae554053c22c454584e9d6c5c94d9ce2ca2e22244c1f06de89e7c4956b57.svg","subcategory":"Data","website":"https://github.com/georgia-tech-db/evadb","description":"Tool to use SQL and database semantics to create AI systems"},{"category":"Technical Prototypes","id":"technical-prototypes--data--linearized-llm","name":"Linearized LLM","logo":"logos/e39ad5ad12a68446946fd8f8181cc0cd6dd9cae71dff69919e4a69e1baf8a86b.svg","subcategory":"Data","website":"https://github.com/GATECH-EIC/Linearized-LLM","description":"ICML 2024 related work focused on improving Llama2 speed and perplexity. When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models"},{"category":"Technical Prototypes","id":"technical-prototypes--data--nested-fusion","name":"Nested Fusion","logo":"logos/4ed9df80dd2205c8ffb81f259f05e35b96c395e03cd3d594ff2938881b27d1ff.svg","subcategory":"Data","website":"https://github.com/pixlise/NestedFusion","description":"AI models for Mars rover data analysis; Dimensioanlity Reduction and Latent Structure Analysis of Multi-Scale Nested Data for M2020 PIXL RGBU and XRF Data"},{"category":"Technical Prototypes","id":"technical-prototypes--data--conscience-cache-challenges","name":"Conscience CACHE Challenges","logo":"logos/e7575d722e9acf3c38a90b4179d6e376401f52c191ee1b5037ebbf3bc7c50edc.svg","subcategory":"Data","website":"https://conscience.ca/cache-challenge/","description":"The CACHE (Critical Assessment of Computation Hit-finding Experiments) Challenges offer an open competition platform to help accelerate one of the early stages of drug discovery. Researchers from academia, industry, and nonprofits are invited to deploy their best computational methods to predict molecules that will bind to a predefined target linked to a specific disease, a critical step in the drug discovery pipeline known as hit-finding. Their predictions are evaluated and benchmarked in a state-of-the-art laboratory, by our partners at the Structural Genomics Consortium (SGC). All the benchmarked results are shared openly and publicly with the world, and all chemical structures are made available without patent to all."},{"category":"Technical Prototypes","id":"technical-prototypes--data--graph-network-simulator","name":"Graph Network Simulator","logo":"logos/1f3aa41a44a6f27e22606029a36772f4685721d05100951cdd2e86e0703fad31.svg","subcategory":"Data","website":"https://www.geoelements.org/gns/#/","description":"Graph Network-based Simulator (GNS) is a framework for developing generalizable, efficient, and accurate machine learning (ML)-based surrogate models for particulate and fluid systems using Graph Neural Networks (GNNs). GNS code is a viable surrogate for numerical methods such as Material Point Method, Smooth Particle Hydrodynamics and Computational Fluid dynamics. GNS exploits distributed data parallelism to achieve fast multi-GPU training. The GNS code can handle complex boundary conditions and multi-material interactions. GNS is a viable surrogate for numerical models such as Material Point Method, Smooth Particle Hydrodynamics and Computational Fluid dynamics."},{"category":"Technical Prototypes","id":"technical-prototypes--data--operator-learning","name":"Operator Learning","logo":"logos/b02c2fdfa394eee6106394e8759962c7534b0ea1486c855d69e1d2659c6d83fc.svg","subcategory":"Data","website":"https://tyler-ingebrand.github.io/OperatorFunctionEncoder/","description":"We present Basis-to-Basis (B2B) operator learning, a novel approach for learning operators on Hilbert spaces of functions based on the foundational ideas of function encoders. We decompose the task of learning operators into two parts, learning sets of basis functions for both the input and output spaces, and learning a potentially nonlinear mapping between the coefficients of the basis functions. B2B operator learning circumvents many challenges of prior works, such as requiring data to be at fixed locations, by leveraging classic techniques such as least-squares to compute the coefficients."},{"category":"Technical Prototypes","id":"technical-prototypes--data--pangea-multilingual-multimodal-large-language-model-mllm","name":"Pangea Multilingual Multimodal Large Language Model (MLLM)","logo":"logos/114ff391c729815ab29a07c2c4b0ba3f6fd1e3c500dd58961a8bcb0526573b63.svg","subcategory":"Data","website":"https://neulab.github.io/Pangea/","description":"Carnegie Mellon University's School of Computer Science developed Pangea. The open-source, multilingual multimodal large language model (MLLM) recognizes 39 languages and was trained on six million data samples to create a culturally inclusive model."}]}