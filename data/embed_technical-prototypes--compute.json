{"category":{"name":"Technical Prototypes","normalized_name":"technical-prototypes","subcategories":[{"name":"Compute","normalized_name":"compute"}]},"foundation":"OFAI","items":[{"category":"Technical Prototypes","id":"technical-prototypes--compute--eva-db","name":"Eva DB","logo":"logos/38b2ae554053c22c454584e9d6c5c94d9ce2ca2e22244c1f06de89e7c4956b57.svg","subcategory":"Compute","website":"https://github.com/georgia-tech-db/evadb","description":"Tool to use SQL and database semantics to create AI systems"},{"category":"Technical Prototypes","id":"technical-prototypes--compute--linearized-llm","name":"Linearized LLM","logo":"logos/e39ad5ad12a68446946fd8f8181cc0cd6dd9cae71dff69919e4a69e1baf8a86b.svg","subcategory":"Compute","website":"https://github.com/GATECH-EIC/Linearized-LLM","description":"ICML 2024 related work focused on improving Llama2 speed and perplexity. When Linear Attention Meets Autoregressive Decoding: Towards More Effective and Efficient Linearized Large Language Models"}]}