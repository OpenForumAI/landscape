{"classification":{"category":{"name":"Technical Prototypes","normalized_name":"technical-prototypes","subcategories":[{"name":"Code","normalized_name":"code"}]}},"foundation":"OFAI","items":[{"category":"Technical Prototypes","id":"technical-prototypes--code--good-retrieval-augmented-generation-grag","name":"Good Retrieval Augmented Generation (GRAG)","logo":"logos/7e36944ab12adc90dccd041679430426cae3b769101df79369620431d06b3930.svg","subcategory":"Code","website":"https://g-rag.org/","description":"A simple open-source python package that provides an easy end-to-end solution for implementing Retrieval Augmented Generation (RAG)"},{"category":"Technical Prototypes","id":"technical-prototypes--code--pangea-multilingual-multimodal-large-language-model-mllm","name":"Pangea Multilingual Multimodal Large Language Model (MLLM)","logo":"logos/114ff391c729815ab29a07c2c4b0ba3f6fd1e3c500dd58961a8bcb0526573b63.svg","subcategory":"Code","website":"https://neulab.github.io/Pangea/","description":"Carnegie Mellon University's School of Computer Science developed Pangea. The open-source, multilingual multimodal large language model (MLLM) recognizes 39 languages and was trained on six million data samples to create a culturally inclusive model."}]}